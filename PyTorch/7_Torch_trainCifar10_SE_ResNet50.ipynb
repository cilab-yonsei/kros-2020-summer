{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_Torch_trainCifar10_SE_ResNet50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "omu4-vsanK3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import sklearn.metrics as skl\n",
        "\n",
        "\n",
        "\n",
        "# Define CNN Architecture\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "#Visualizing CIFAR 10\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data',download=True)\n",
        "datanum = len(trainset)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "fig = plt.figure()\n",
        "ims = np.random.randint(datanum, size=15)\n",
        "\n",
        "for i in range(15):\n",
        "    subplot = fig.add_subplot(3,5, i+1)\n",
        "    subplot.set_xticks([])\n",
        "    subplot.set_yticks([])\n",
        "    PILimg, label = trainset[ims[i]]\n",
        "    subplot.set_title(\"%s\" %classes[label])\n",
        "    subplot.imshow(PILimg)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhtVibcI9PsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class SE_ResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SE_ResNet50, self).__init__()\n",
        "\n",
        "        self.conv1 = self.conv1_layer()\n",
        "        self.conv2 = self.conv2_layer()\n",
        "        self.conv3 = self.conv3_layer()\n",
        "        self.conv4 = self.conv4_layer()\n",
        "        self.conv5 = self.conv5_layer()\n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "\n",
        "\n",
        "    def conv1_layer(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "\n",
        "    def conv2_layer(self):\n",
        "        layers = []\n",
        "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        layers.append(conv_block(in_channels=64, filters = [64,64,256], strides=1))\n",
        "        layers.append(identity_block(filters = [64,64,256]))\n",
        "        layers.append(identity_block(filters = [64,64,256]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def conv3_layer(self):\n",
        "        layers = []\n",
        "        layers.append(conv_block(in_channels=256, filters = [128,128,512], strides=2))\n",
        "        layers.append(identity_block(filters = [128,128,512]))\n",
        "        layers.append(identity_block(filters = [128,128,512]))\n",
        "        layers.append(identity_block(filters = [128,128,512]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def conv4_layer(self):\n",
        "        layers = []\n",
        "        layers.append(conv_block(in_channels=512, filters = [256,256,1024], strides=2))\n",
        "        layers.append(identity_block(filters = [256,256,1024]))\n",
        "        layers.append(identity_block(filters = [256,256,1024]))\n",
        "        layers.append(identity_block(filters = [256,256,1024]))\n",
        "        layers.append(identity_block(filters = [256,256,1024]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def conv5_layer(self):\n",
        "        layers = []\n",
        "        layers.append(conv_block(in_channels=1024,filters = [512,512,2048], strides=2))\n",
        "        layers.append(identity_block(filters = [512,512,2048]))\n",
        "        layers.append(identity_block(filters = [512,512,2048]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = F.avg_pool2d(out, kernel_size=1)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, filters, strides):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.filters1, self.filters2, self.filters3 = filters\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.filters1, kernel_size=1, stride=strides, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.filters1)\n",
        "        self.conv2 = nn.Conv2d(self.filters1, self.filters2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(self.filters2)\n",
        "        self.conv3 = nn.Conv2d(self.filters2, self.filters3, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.filters3)\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.filters3, kernel_size=1, stride=strides, bias=False),\n",
        "            nn.BatchNorm2d(self.filters3)\n",
        "        )\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Conv2d(self.filters3, self.filters3//16, kernel_size=1),  # Use nn.Conv2d instead of nn.Linear\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Conv2d(self.filters3//16, self.filters3, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        # Squeeze\n",
        "        w = F.avg_pool2d(out, kernel_size=out.size(2))\n",
        "        w = self.fc1(w)\n",
        "        w = self.fc2(w)\n",
        "        # Excitation\n",
        "        out = out * w  # New broadcasting feature from v0.2!\n",
        "\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class identity_block(nn.Module):\n",
        "\n",
        "    def __init__(self, filters):\n",
        "        super(identity_block, self).__init__()\n",
        "        self.filters1, self.filters2, self.filters3 = filters\n",
        "        self.conv1 = nn.Conv2d(self.filters3, self.filters1, kernel_size=1, stride=1, padding=0, bias=False)#padding=0=valid\n",
        "        self.bn1 = nn.BatchNorm2d(self.filters1)\n",
        "        self.conv2 = nn.Conv2d(self.filters1, self.filters2, kernel_size=3, stride=1, padding=1, bias=False)#padding=1=same\n",
        "        self.bn2 = nn.BatchNorm2d(self.filters2)\n",
        "        self.conv3 = nn.Conv2d(self.filters2, self.filters3, kernel_size=1, stride=1, padding=0, bias=False)#padding=0=valid\n",
        "        self.bn3 = nn.BatchNorm2d(self.filters3)\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Conv2d(self.filters3, self.filters3//16, kernel_size=1),  # Use nn.Conv2d instead of nn.Linear\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Conv2d(self.filters3//16, self.filters3, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        # Squeeze\n",
        "        w = F.avg_pool2d(out, kernel_size=out.size(2))\n",
        "        w = self.fc1(w)\n",
        "        w = self.fc2(w)\n",
        "        # Excitation\n",
        "        out = out * w  # New broadcasting feature from v0.2!\n",
        "\n",
        "        out += x\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "print('==> Building model..')\n",
        "net = SE_ResNet50()\n",
        "\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    print('\\nTrain:')\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print_time = -1\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if time.time()-print_time > 1 or print_time == -1 or batch_idx+1 == len(trainloader):\n",
        "            print('[%3d/%3d] | Loss: %.3f | Acc: %.3f%% (%d/%d)'%(\n",
        "                batch_idx+1, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            \n",
        "            print_time = time.time()\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\nTest:')\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print_time = -1\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if time.time()-print_time > 1 or print_time == -1 or batch_idx+1 == len(testloader):\n",
        "                print('[%3d/%3d] | Loss: %.3f | Acc: %.3f%% (%d/%d)'%(\n",
        "                    batch_idx+1, len(testloader), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "                \n",
        "                print_time = time.time()\n",
        "                \n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TztY64oz1_jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ4FmLMHpER2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ylabel = []\n",
        "yhatlabel = []\n",
        "\n",
        "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = outputs.max(1)\n",
        "    ylabel = np.concatenate((ylabel, targets.cpu().numpy()))\n",
        "    yhatlabel = np.concatenate((yhatlabel, predicted.cpu().numpy()))\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = skl.confusion_matrix(ylabel, yhatlabel)\n",
        "np.set_printoptions(precision=2)\n",
        "is_correct = (ylabel == yhatlabel)\n",
        "acc = np.sum(is_correct * 1) / len(is_correct)\n",
        "print('accuracy:%.5f' %acc)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                  title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
        "                  title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}